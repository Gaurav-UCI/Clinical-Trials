{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Trial & Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import streamlit as st\n",
    "from langchain import LLMChain, OpenAI\n",
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(file):\n",
    "    \n",
    "    pdf = PdfReader(file)\n",
    "    output = []\n",
    "\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "        output.append(text)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_docs(text):\n",
    "  \n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    page_docs = [Document(page_content=page) for page in text]\n",
    "\n",
    "    for i, doc in enumerate(page_docs):\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "    doc_chunks = []\n",
    "\n",
    "    for doc in page_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "            )\n",
    "            doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "            doc_chunks.append(doc)\n",
    "\n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../openai-key/openai_key.txt\", 'r') as file:\n",
    "    api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0, model = 'text-davinci-003', openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain({\"question\": \"Provide me a description of this clinical trial\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\Data Technology & Fellowship\\clinical-trial-matching-master\\Clinical-Trails Testing\\PDF\\ICF CCR_20-41.pdf\"\n",
    "path = path.split(\"\\\\\")\n",
    "print(path)\n",
    "print(path[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import magic\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=api_key, model=\"davinci-002\")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=index.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello! How are you?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                chain_type=\"stuff\",\n",
    "                                retriever=index.as_retriever(),\n",
    "                                return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the study sponsor, and what responsibilities do they have in relation to the study?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "import pickle\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(file):\n",
    "    pdf = PdfReader(file)\n",
    "    output = []\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "        text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "        output.append(text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = parse_pdf(r\"PDF\\ICF CCR_20-41.pdf\")\n",
    "print(output)\n",
    "if \"Protocol Number\" in output[0]:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_protocol_number = \"CCR-20-41 and\"\n",
    "\n",
    "modified_text = output[0]\n",
    "\n",
    "protocol_index = modified_text.find(\"Protocol Number\")\n",
    "\n",
    "while protocol_index != -1:\n",
    "    \n",
    "    existing_text = modified_text[:protocol_index]\n",
    "    remaining_text = modified_text[protocol_index:]\n",
    "    \n",
    "    modified_text = existing_text + remaining_text.replace(\"Protocol Number\", f\"Protocol Number {new_protocol_number}\", 1)\n",
    "     \n",
    "    protocol_index = modified_text.find(\"Protocol Number\", protocol_index + len(f\"Protocol Number {new_protocol_number}\") + 1)\n",
    "    \n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_docs(text, filename):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    page_docs = [Document(page_content=page) for page in text]\n",
    "    for i, doc in enumerate(page_docs):\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "    doc_chunks = []\n",
    "    for doc in page_docs:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=4000,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "            chunk_overlap=0,\n",
    "        )\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "            )\n",
    "            doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "            doc.metadata[\"filename\"] = filename \n",
    "            doc_chunks.append(doc)\n",
    "            \n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_index(docs, openai_api_key):\n",
    "    index = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_index_for_pdf(directory_path, openai_api_key):\n",
    "    \n",
    "    pdf_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith(\".pdf\")]\n",
    "    documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        \n",
    "        filename = pdf_file.split(\"\\\\\")\n",
    "        filename = filename[-1]\n",
    "        text = parse_pdf(pdf_file)\n",
    "        documents = documents + text_to_docs(text, filename)\n",
    "    index = docs_to_index(documents, openai_api_key)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"C:\\Users\\gaura\\OneDrive\\Documents\\Data Technology & Fellowship\\clinical-trial-matching-master\\Clinical-Trails Testing\\PDF\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"openai_key.txt\", 'r') as file:\n",
    "    api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = get_index_for_pdf(folder,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the background and purpose of the study? please give a big answer.\"\n",
    "docs = vectordb.similarity_search(query,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databutton as db\n",
    "import re\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "import pickle\n",
    "import requests\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.schema import ( SystemMessage, HumanMessage, AIMessage)\n",
    "from PyPDF2 import PdfReader\n",
    "import faiss\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\gaura\\OneDrive\\Documents\\DTF\\clinical-trial-matching-master\\Clinical-Trails\\cardio_trials.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"openai_key.txt\", 'r') as file:\n",
    "    api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for protocol in data['TRIAL']['PROTOCOL']:\n",
    "        text = \"Protocol No: \" + protocol[\"PROTOCOL_NO\"] + \" \"\n",
    "        text += \"Title: \" + protocol[\"TITLE\"] + \" \"\n",
    "        text += \"NCT ID: \" + protocol[\"NCT_ID\"] + \" \"\n",
    "        text += \"Short Title: \" + protocol[\"SHORT_TITLE\"] + \" \"\n",
    "        text += \"Investigator Name: \" + protocol[\"INVESTIGATOR_NAME\"] + \" \"\n",
    "        text += \"Status: \" + protocol[\"STATUS\"] + \" \"\n",
    "        text += \"Elibility: \" + protocol[\"ELIGIBILITY\"] + \" \"\n",
    "        text += \"Detailed Eligibility: \" + protocol[\"DETAILED_ELIGIBILITY\"] + \" \"\n",
    "        text += \"Age Description: \" + protocol[\"AGE_DESCRIPTION\"] + \" \"\n",
    "        text += \"Phase Desc: \" + protocol[\"PHASE_DESC\"] + \" \"\n",
    "        text += \"Scope Description: \"+ protocol[\"SCOPE_DESC\"] + \" \"\n",
    "        text += \"Modified Date: \"+ protocol[\"MODIFIED_DATE\"] + \" \"\n",
    "        text += \"Department Name: \" + protocol[\"DEPARTMENT_NAME\"] + \" \"\n",
    "        text += \"Sponsor Names: \" + str(protocol[\"SPONSOR_NAMES\"]) + \" \"\n",
    "        text += \"Disease Sites: \"+ str(protocol[\"DISEASE_SITES\"]) + \" \"\n",
    "        docs.append(Document(page_content=text, metadata={\"Protocol No\":protocol[\"PROTOCOL_NO\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"],model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(chat, messages, query):\n",
    "    \n",
    "    with open(r\"openai_key.txt\", 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "    db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "    results = db.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query. Contexts: {source_knowledge} Query: {query}\"\"\"\n",
    "    prompt =  HumanMessage(content=augmented_prompt)\n",
    "    messages.append(prompt)\n",
    "    result = chat(messages)\n",
    "    messages.append(AIMessage(content=result.content))\n",
    "\n",
    "    return messages, result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Which clinical trial is sponsored by Boston Scientific'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"), \n",
    "            HumanMessage(content=\"Hi AI, how are you today?\"), \n",
    "            AIMessage(content=\"I'm great thank you. How can I help you?\")\n",
    "        ]   \n",
    "messages, bot_answer = generate_responses(chat, messages,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\gaura\\OneDrive\\Documents\\DTF\\clinical-trial-matching-master\\Clinical-Trails\\cardio_trials.json'\n",
    "protocol = data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for protocol in data['TRIAL']['PROTOCOL']:\n",
    "    print(\"Protocol No:\", protocol[\"PROTOCOL_NO\"])\n",
    "    print(\"Title:\", protocol[\"TITLE\"])\n",
    "    print(\"NCT ID:\", protocol[\"NCT_ID\"])\n",
    "    print(\"Short Title:\", protocol[\"SHORT_TITLE\"])\n",
    "    print(\"Investigator Name:\", protocol[\"INVESTIGATOR_NAME\"])\n",
    "    print(\"Status:\", protocol[\"STATUS\"])\n",
    "    print(\"Age Description:\", protocol[\"AGE_DESCRIPTION\"])\n",
    "    print(\"Scope Description:\", protocol[\"SCOPE_DESC\"])\n",
    "    print(\"Description:\", protocol[\"DESCRIPTION\"])\n",
    "    print(\"Sponsor Names:\", protocol[\"SPONSOR_NAMES\"])\n",
    "    print(\"Disease Sites:\", protocol[\"DISEASE_SITES\"])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'Which clinical trial is sponsored by Boston Scientific'\n",
    "sentence2 = 'Which clinical trial is related to AstraZeneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed1 = openai.Embedding.create(input = [sentence1, sentence2], engine=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = embed1[\"data\"][0][\"embedding\"]\n",
    "second = embed1[\"data\"][1][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity\n",
    "score = cosine_similarity(first,second)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_key.txt\",\"r\") as file:\n",
    "    api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Policies Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import faiss\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_db:\n",
    "\n",
    "    def parse_pdf( self, file, filename, idx):\n",
    "\n",
    "        pdf = PdfReader(file)\n",
    "        print(file)\n",
    "        print(self.dictionary[idx])\n",
    "        output = []\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            text = self.replace_text(text,idx)\n",
    "            text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "            text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "            text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "            output.append(text)\n",
    "        return output, filename\n",
    "\n",
    "    def replace_text(self, text, idx):\n",
    "        \n",
    "        new_protocol_number = str(self.dictionary[idx]) + \" and\"\n",
    "        modified_text = text\n",
    "        protocol_index = modified_text.find(\"Protocol Number\")\n",
    "\n",
    "        while protocol_index != -1:\n",
    "    \n",
    "            existing_text = modified_text[:protocol_index]\n",
    "            remaining_text = modified_text[protocol_index:]\n",
    "            modified_text = existing_text + remaining_text.replace(\"Protocol Number\", f\"Protocol Number {new_protocol_number}\", 1)\n",
    "            protocol_index = modified_text.find(\"Protocol Number\", protocol_index + len(f\"Protocol Number {new_protocol_number}\") + 1)\n",
    "\n",
    "        return modified_text   \n",
    "    \n",
    "    def text_to_docs( self, text, filename):\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        page_docs = [Document(page_content=page) for page in text]\n",
    "        for i, doc in enumerate(page_docs):\n",
    "            doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "        doc_chunks = []\n",
    "        for doc in page_docs:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=4000,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "                chunk_overlap=0,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc = Document(\n",
    "                    page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "                )\n",
    "                doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "                doc.metadata[\"filename\"] = filename  # Add filename to metadata\n",
    "                doc_chunks.append(doc)\n",
    "        return doc_chunks\n",
    "\n",
    "\n",
    "    def docs_to_index( self, docs, api_key):\n",
    "        \n",
    "        db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "        db.save_local(\"Vector_DB/policies\")\n",
    "\n",
    "\n",
    "    def get_index_for_pdf( self, folder_path, api_key, dictionary):\n",
    "        \n",
    "        pdf_files, pdf_names = [], []\n",
    "        documents = []\n",
    "        self.dictionary = dictionary\n",
    "        files = os.listdir(folder_path)\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_files.append(os.path.join(folder_path, file))\n",
    "            pdf_names.append(file)\n",
    "\n",
    "        idx = 0\n",
    "        for pdf_file, pdf_name in zip(pdf_files, pdf_names):\n",
    "            text, filename = self.parse_pdf(pdf_file, pdf_name, idx)\n",
    "            documents = documents + self.text_to_docs(text, filename)\n",
    "            idx+=1\n",
    "        self.docs_to_index(documents, api_key)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Create_db()\n",
    "folder_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\DTF\\clinical-trial-matching-master\\Clinical-Trails\\PDF\"\n",
    "dictionary = [\"CCR-20-41\", 'CCR-21-66', \"CCR-22-101\", \"CCR-22-13\", \"CCR-22-96\", \"CCR-23-06\"]\n",
    "database.get_index_for_pdf(folder_path, api_key, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML File Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_db:\n",
    "\n",
    "    def generate_docs(self, data):\n",
    "        docs = []\n",
    "        for protocol in data['TRIAL']['PROTOCOL']:\n",
    "                text = \"Protocol No: \" + protocol[\"PROTOCOL_NO\"] + \" \"\n",
    "                text += \"Title: \" + protocol[\"TITLE\"] + \" \"\n",
    "                text += \"NCT ID: \" + protocol[\"NCT_ID\"] + \" \"\n",
    "                text += \"Short Title: \" + protocol[\"SHORT_TITLE\"] + \" \"\n",
    "                text += \"Investigator Name: \" + protocol[\"INVESTIGATOR_NAME\"] + \" \"\n",
    "                text += \"Status: \" + protocol[\"STATUS\"] + \" \"\n",
    "                text += \"Elibility: \" + protocol[\"ELIGIBILITY\"] + \" \"\n",
    "                text += \"Detailed Eligibility: \" + protocol[\"DETAILED_ELIGIBILITY\"] + \" \"\n",
    "                text += \"Age Description: \" + protocol[\"AGE_DESCRIPTION\"] + \" \"\n",
    "                text += \"Phase Desc: \" + protocol[\"PHASE_DESC\"] + \" \"\n",
    "                text += \"Scope Description: \"+ protocol[\"SCOPE_DESC\"] + \" \"\n",
    "                text += \"Modified Date: \"+ protocol[\"MODIFIED_DATE\"] + \" \"\n",
    "                text += \"Department Name: \" + protocol[\"DEPARTMENT_NAME\"] + \" \"\n",
    "                text += \"Sponsor Names: \" + str(protocol[\"SPONSOR_NAMES\"]) + \" \"\n",
    "                text += \"Disease Sites: \"+ str(protocol[\"DISEASE_SITES\"]) + \" \"\n",
    "                docs.append(Document(page_content=text, metadata={\"source\": protocol[\"PROTOCOL_NO\"]}))\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def docs_to_index( self, docs, api_key):\n",
    "        db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "        db.save_local(\"Vector_DB/xml_db\")\n",
    "\n",
    "\n",
    "    def create_index(self, file, api_key):\n",
    "        data = json.loads(Path(file).read_text())\n",
    "        docs = self.generate_docs(data)\n",
    "        self.docs_to_index(docs,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Create_db()\n",
    "folder_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\DTF\\clinical-trial-matching-master\\Clinical-Trails\\cardio_trials.json\"\n",
    "database.create_index(folder_path,api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Policies & Files Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from io import BytesIO\n",
    "from typing import Tuple, List\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_db:\n",
    "    \n",
    "    def parse_pdf( self, file, filename, idx):\n",
    "    \n",
    "        pdf = PdfReader(file)\n",
    "        print(file)\n",
    "        print(self.dictionary[idx])\n",
    "        output = []\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            text = self.replace_text(text,idx)\n",
    "            text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "            text = re.sub(r\"(?<!\\n\\s)\\n(?!\\s\\n)\", \" \", text.strip())\n",
    "            text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "            print(text)\n",
    "            output.append(text)\n",
    "        return output, filename\n",
    "\n",
    "    def replace_text(self, text, idx):\n",
    "        \n",
    "        new_protocol_number = str(self.dictionary[idx]) + \" and\"\n",
    "        modified_text = text\n",
    "        protocol_index = modified_text.find(\"Protocol Number\")\n",
    "\n",
    "        while protocol_index != -1:\n",
    "    \n",
    "            existing_text = modified_text[:protocol_index]\n",
    "            remaining_text = modified_text[protocol_index:]\n",
    "            modified_text = existing_text + remaining_text.replace(\"Protocol Number\", f\"Protocol Number {new_protocol_number}\", 1)\n",
    "            protocol_index = modified_text.find(\"Protocol Number\", protocol_index + len(f\"Protocol Number {new_protocol_number}\") + 1)\n",
    "\n",
    "        return modified_text\n",
    "    \n",
    "    def generate_docs(self, documents, data):\n",
    "        for protocol in data['TRIAL']['PROTOCOL']:\n",
    "                text = \"Protocol No: \" + protocol[\"PROTOCOL_NO\"] + \" \"\n",
    "                text += \"Title: \" + protocol[\"TITLE\"] + \" \"\n",
    "                text += \"NCT ID: \" + protocol[\"NCT_ID\"] + \" \"\n",
    "                text += \"Short Title: \" + protocol[\"SHORT_TITLE\"] + \" \"\n",
    "                text += \"Investigator Name: \" + protocol[\"INVESTIGATOR_NAME\"] + \" \"\n",
    "                text += \"Status: \" + protocol[\"STATUS\"] + \" \"\n",
    "                text += \"Elibility: \" + protocol[\"ELIGIBILITY\"] + \" \"\n",
    "                text += \"Detailed Eligibility: \" + protocol[\"DETAILED_ELIGIBILITY\"] + \" \"\n",
    "                text += \"Age Description: \" + protocol[\"AGE_DESCRIPTION\"] + \" \"\n",
    "                text += \"Phase Desc: \" + protocol[\"PHASE_DESC\"] + \" \"\n",
    "                text += \"Scope Description: \"+ protocol[\"SCOPE_DESC\"] + \" \"\n",
    "                text += \"Modified Date: \"+ protocol[\"MODIFIED_DATE\"] + \" \"\n",
    "                text += \"Department Name: \" + protocol[\"DEPARTMENT_NAME\"] + \" \"\n",
    "                text += \"Sponsor Names: \" + str(protocol[\"SPONSOR_NAMES\"]) + \" \"\n",
    "                text += \"Disease Sites: \"+ str(protocol[\"DISEASE_SITES\"]) + \" \"\n",
    "                documents.append(Document(page_content=text, metadata={\"source\": protocol[\"PROTOCOL_NO\"]}))\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def text_to_docs( self, text, filename):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        page_docs = [Document(page_content=page) for page in text]\n",
    "        for i, doc in enumerate(page_docs):\n",
    "            doc.metadata[\"page\"] = i + 1\n",
    "\n",
    "        doc_chunks = []\n",
    "        for doc in page_docs:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=4000,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "                chunk_overlap=0,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc = Document(\n",
    "                    page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "                )\n",
    "                doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "                doc.metadata[\"filename\"] = filename  # Add filename to metadata\n",
    "                doc_chunks.append(doc)\n",
    "        return doc_chunks\n",
    "\n",
    "\n",
    "    def docs_to_index( self, docs, api_key):\n",
    "        db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "        db.save_local(\"Vector_DB/main_db\")\n",
    "\n",
    "\n",
    "    def get_index_for_pdf( self, folder_path, file_path, api_key,dictionary):\n",
    "        pdf_files, pdf_names = [], []\n",
    "        documents = []\n",
    "        self.dictionary = dictionary\n",
    "        files = os.listdir(folder_path)\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_files.append(os.path.join(folder_path, file))\n",
    "            pdf_names.append(file)\n",
    "\n",
    "        idx=0\n",
    "        for pdf_file, pdf_name in zip(pdf_files, pdf_names):\n",
    "            text, filename = self.parse_pdf(pdf_file, pdf_name, idx)\n",
    "            documents = documents + self.text_to_docs(text, filename)\n",
    "            idx+=1\n",
    "            \n",
    "        data = json.loads(Path(file_path).read_text())\n",
    "        \n",
    "        documents = self.generate_docs( documents, data)\n",
    "        self.docs_to_index(documents, api_key)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Create_db()\n",
    "file_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\DTF\\clinical-trial-matching-master\\Clinical-Trails\\cardio_trials.json\"\n",
    "folder_path = r\"C:\\Users\\gaura\\OneDrive\\Documents\\DTF\\clinical-trial-matching-master\\Clinical-Trails\\PDF\"\n",
    "dictionary = [\"CCR-20-41\", 'CCR-21-66', \"CCR-22-101\", \"CCR-22-13\", \"CCR-22-96\", \"CCR-23-06\"]\n",
    "database.get_index_for_pdf( folder_path, file_path, api_key, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Json Data Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_db:\n",
    "    \n",
    "    def generate_docs(self, data):\n",
    "        docs = []\n",
    "        for protocol in data:\n",
    "                text = \"NCT ID: \" + protocol[\"NCT_ID\"] + \" \"\n",
    "                text += \"Title: \" + protocol[\"TITLE\"] + \" \"\n",
    "                text += \"Short Title: \" + protocol[\"SHORT_TITLE\"] + \" \"\n",
    "                text += \"Sponsor: \" + protocol[\"SPONSOR\"] + \" \"\n",
    "                text += \"Detailed Eligibility: \" + protocol[\"DETAILED_ELIGIBILITY\"] + \" \"\n",
    "                if \"DESCRIPTION\" in protocol:\n",
    "                    text += \"Description: \" + protocol[\"DESCRIPTION\"] + \" \"\n",
    "                text += \"Summary: \" + protocol[\"SUMMARY\"] + \" \"\n",
    "                text += \"Status: \" + protocol[\"STATUS\"] + \" \"\n",
    "                if  \"OUTCOME_DESCRIPTION\" in protocol:\n",
    "                    text += \"Outcome Description: \" + protocol[\"OUTCOME_DESCRIPTION\"] + \" \"\n",
    "                if \"OUTCOME_MEASURE\" in protocol:\n",
    "                    text += \"Outcome Measure: \" + protocol[\"OUTCOME_MEASURE\"] + \" \"\n",
    "                if \"OUTCOME_TIMEFRAME\" in protocol:\n",
    "                    text += \"Outcome Timeframe: \" + protocol[\"OUTCOME_TIMEFRAME\"] + \" \"\n",
    "                text += \"Age Description: \" + protocol[\"AGE_DESCRIPTION\"] + \" \"\n",
    "                if \"INVESTIGATOR_NAME\" in protocol:\n",
    "                    text += \"Investigator Name: \" + protocol[\"INVESTIGATOR_NAME\"]+ \" \"\n",
    "                docs.append(Document(page_content=text, metadata={\"source\": protocol[\"NCT_ID\"]}))\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def docs_to_index( self, docs, api_key):\n",
    "        db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "        db.save_local(\"vectorDB/mainDB\")\n",
    "\n",
    "\n",
    "    def create_index(self, file, api_key):\n",
    "        data = json.loads(Path(file).read_text())\n",
    "        docs = self.generate_docs(data)\n",
    "        self.docs_to_index(docs,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_db:\n",
    "    \n",
    "    def generate_docs(self, data):\n",
    "        docs = []\n",
    "        for protocol in data:\n",
    "            text = protocol[\"NCT_ID\"] + \" \"\n",
    "            if \"TITLE\" in protocol:\n",
    "                text += \"Title: \" + protocol[\"TITLE\"] + \" \"\n",
    "            text += \"Short Title: \" + protocol[\"SHORT_TITLE\"] + \" \"\n",
    "            text += \"Sponsor: \" + protocol[\"SPONSOR\"] + \" \"\n",
    "            text += \"Detailed Eligibility: \" + protocol[\"DETAILED_ELIGIBILITY\"] + \" \"\n",
    "            if \"DESCRIPTION\" in protocol:\n",
    "                text += \"Description: \" + protocol[\"DESCRIPTION\"] + \" \"\n",
    "            text += \"Summary: \" + protocol[\"SUMMARY\"] + \" \"\n",
    "            text += \"Status: \" + protocol[\"STATUS\"] + \" \"\n",
    "            if  \"PRIMARY_OUTCOMES\" in protocol:\n",
    "                text += \"Outcome Description: \" + protocol[\"PRIMARY_OUTCOMES\"] + \" \"\n",
    "            if \"SECONDARY_OUTCOMES\" in protocol:\n",
    "                text += \"Outcome Measure: \" + protocol[\"SECONDARY_OUTCOMES\"] + \" \"\n",
    "            if \"OTHER_OUTCOMES\" in protocol:\n",
    "                text += \"Outcome Timeframe: \" + protocol[\"OTHER_OUTCOMES\"] + \" \"\n",
    "            text += \"Age Description: \" + protocol[\"AGE_DESCRIPTION\"] + \" \"\n",
    "            if \"CONDITIONS\" in protocol:\n",
    "                text += \"Conditions: \" + str(protocol[\"CONDITIONS\"])+ \" \"\n",
    "            if \"OVERALL_OFFICIALS\" in protocol:\n",
    "                text += \"Overall Officials: \" + protocol[\"OVERALL_OFFICIALS\"]+ \" \"\n",
    "            if \"LOCATIONS\" in protocol:\n",
    "                text += \"Locations: \" + protocol[\"LOCATIONS\"]+ \" \"\n",
    "            \n",
    "            docs.append(Document(page_content=text, metadata={\"source\": protocol[\"NCT_ID\"]}))\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def docs_to_index( self, docs, api_key):\n",
    "        db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "        db.save_local(\"vectorDB/primaryDB\")\n",
    "\n",
    "\n",
    "    def create_index(self, file, api_key):\n",
    "        data = json.loads(Path(file).read_text())\n",
    "        docs = self.generate_docs(data)\n",
    "        self.docs_to_index(docs,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_db:\n",
    "    \n",
    "    def generate_docs(self, data):\n",
    "        docs = []\n",
    "        for protocol in data:\n",
    "            text = protocol[\"NCT_ID\"] + \" \"\n",
    "            if \"TITLE\" in protocol:\n",
    "                text += protocol[\"TITLE\"] + \" \"\n",
    "            text += protocol[\"SHORT_TITLE\"] + \" \"\n",
    "            text += protocol[\"SPONSOR\"] + \" \"\n",
    "            # print(protocol[\"ORGANIZATION\"])\n",
    "            if \"ORGANIZATION\" in protocol:\n",
    "                text += protocol[\"ORGANIZATION\"] + \" \"\n",
    "            text += protocol[\"DETAILED_ELIGIBILITY\"] + \" \"\n",
    "            if \"DESCRIPTION\" in protocol:\n",
    "                text += protocol[\"DESCRIPTION\"] + \" \"\n",
    "            text += protocol[\"SUMMARY\"] + \" \"\n",
    "            text += protocol[\"STATUS\"] + \" \"\n",
    "            if  \"PRIMARY_OUTCOMES\" in protocol:\n",
    "                text += protocol[\"PRIMARY_OUTCOMES\"] + \" \"\n",
    "            if \"SECONDARY_OUTCOMES\" in protocol:\n",
    "                text += protocol[\"SECONDARY_OUTCOMES\"] + \" \"\n",
    "            if \"OTHER_OUTCOMES\" in protocol:\n",
    "                text += protocol[\"OTHER_OUTCOMES\"] + \" \"\n",
    "            text += protocol[\"AGE_DESCRIPTION\"] + \" \"\n",
    "            if \"CONDITIONS\" in protocol:\n",
    "                text += str(protocol[\"CONDITIONS\"])+ \" \"\n",
    "            if \"OVERALL_OFFICIALS\" in protocol:\n",
    "                text += protocol[\"OVERALL_OFFICIALS\"]+ \" \"\n",
    "            if \"LOCATIONS\" in protocol:\n",
    "                text += str(protocol[\"LOCATIONS\"])+ \" \"\n",
    "            \n",
    "            docs.append(Document(page_content=text, metadata={\"source\": protocol[\"NCT_ID\"]}))\n",
    "        return docs\n",
    "    \n",
    "    def docs_to_index( self, docs, api_key):\n",
    "        db = FAISS.from_documents(docs, OpenAIEmbeddings(openai_api_key=api_key))\n",
    "        db.save_local(\"vectorDB/UCDB1\")\n",
    "\n",
    "\n",
    "    def create_index(self, file, api_key):\n",
    "        data = json.loads(Path(file).read_text())\n",
    "        docs = self.generate_docs(data)\n",
    "        self.docs_to_index(docs,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Create_db()\n",
    "folder_path = r\"../database/protocolDB/ucspecific-1.json\"\n",
    "database.create_index(folder_path,api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.schema import ( SystemMessage, HumanMessage, AIMessage)\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI( openai_api_key = os.environ[\"OPENAI_API_KEY\"], model = 'gpt-3.5-turbo')\n",
    "\n",
    "def generate_responses( chat, messages, faiss_path, query):\n",
    "    \n",
    "    with open(r\"openai_key.txt\", 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "    db = FAISS.load_local(faiss_path, embeddings)\n",
    "\n",
    "    results = db.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query. Contexts: {source_knowledge} Query: {query}\"\"\"\n",
    "    \n",
    "    prompt =  HumanMessage(content=augmented_prompt)\n",
    "    messages.append(prompt)\n",
    "    result = chat(messages)\n",
    "    messages.append(AIMessage(content=result.content))\n",
    "\n",
    "    return messages, result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Policies Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Which clinical trial is sponsored by Boston Scientific'\n",
    "\n",
    "messages = [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"), \n",
    "            HumanMessage(content=\"Hi AI, how are you today?\"), \n",
    "            AIMessage(content=\"I'm great thank you. How can I help you?\")\n",
    "        ]   \n",
    "\n",
    "messages, bot_answer = generate_responses( chat, messages, r\"Vector_DB\\policies\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML File Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Which clinical trial is sponsored by Boston Scientific'\n",
    "\n",
    "messages = [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"), \n",
    "            HumanMessage(content=\"Hi AI, how are you today?\"), \n",
    "            AIMessage(content=\"I'm great thank you. How can I help you?\")\n",
    "        ]   \n",
    "\n",
    "messages, bot_answer = generate_responses( chat, messages, r\"Vector_DB\\xml_db\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Which clinical trial is sponsored by Boston Scientific'\n",
    "\n",
    "messages = [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"), \n",
    "            HumanMessage(content=\"Hi AI, how are you today?\"), \n",
    "            AIMessage(content=\"I'm great thank you. How can I help you?\")\n",
    "        ]   \n",
    "\n",
    "messages, bot_answer = generate_responses( chat, messages, r\"Vector_DB\\xml_db\", question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Trials API Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data has been successfully stored in 'study_data.json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"https://clinicaltrials.gov/api/v2\"\n",
    "\n",
    "nct_id = \"NCT04790344\"\n",
    "\n",
    "url = f\"{base_url}/studies/{nct_id}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    with open(\"study_data.json\", \"w\") as json_file:\n",
    "        json.dump(data, json_file)\n",
    "    \n",
    "    print(\"JSON data has been successfully stored in 'study_data.json'\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCT ID Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gshipurk/Documents/Clinical Trials Github/Clinical-Trials/notebook\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(Path(\".\").absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('../database/clinical_trials.xml') as f:\n",
    "    data = f.read()\n",
    "\n",
    "soup = BeautifulSoup(data, 'xml')\n",
    "ids = soup.find_all('NCT_ID')\n",
    "\n",
    "nct_ids = [] \n",
    "for item in ids:   \n",
    "    item = str(item)\n",
    "    if item.startswith(\"<NCT_ID>\"):\n",
    "        nct_id = item[len(\"<NCT_ID>\"):-len(\"</NCT_ID>\")]\n",
    "        nct_ids.append(nct_id)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame({\"NCT_ID\": nct_ids})\n",
    "\n",
    "df.to_csv(\"../database/nctID-DB/nct_ids.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCT ID Extraction( Based on Trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"https://clinicaltrials.gov/api/v2\"\n",
    "endpoint = \"/studies\"\n",
    "nctids = []\n",
    "count = 0 \n",
    "filters = {\n",
    "    \"query.term\": \"University of California, Irvine\",\n",
    "    \"pageSize\": \"10000\",\n",
    "}\n",
    "url = f\"{base_url}{endpoint}\"\n",
    "\n",
    "response = requests.get(url, params=filters)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    for study in data['studies']:\n",
    "        nctids.append(study['protocolSection']['identificationModule']['nctId'])\n",
    "\n",
    "    next_page_token = data.get(\"nextPageToken\", None)\n",
    "    \n",
    "    while next_page_token or count==5:\n",
    "        filters[\"pageToken\"] = next_page_token\n",
    "        response = requests.get(url, params=filters)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for study in data['studies']:\n",
    "                nctids.append(study['protocolSection']['identificationModule']['nctId'])\n",
    "            \n",
    "            next_page_token = data.get(\"nextPageToken\", None)\n",
    "        else:\n",
    "            print(f\"Error fetching data for next page. Status code: {response.status_code}\")\n",
    "            break\n",
    "    \n",
    "        count+=1\n",
    "else:\n",
    "    print(f\"Error fetching data. Status code: {response.status_code}\")\n",
    "\n",
    "nctids = list(set(nctids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_ids = random.sample(nctids,2000)\n",
    "len(random_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2297"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nctids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_nctids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nctids.extend(nct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2559"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nctids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_nctids = list(set(main_nctids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4195"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_nctids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4195"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_nctids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_ids = random.sample(nctids,2000)\n",
    "len(random_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nct_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m random_ids\u001b[38;5;241m.\u001b[39mextend(\u001b[43mnct_ids\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nct_ids' is not defined"
     ]
    }
   ],
   "source": [
    "random_ids.extend(nct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2558"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nctids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data Based on NCT ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"https://clinicaltrials.gov/api/v2\"\n",
    "trials_info = []\n",
    "for idx, nct_id in enumerate(nctids):\n",
    "    print(idx+1)\n",
    "    url = f\"{base_url}/studies/{nct_id}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            protocol = {}\n",
    "            protocol['NCT_ID'] = nct_id    \n",
    "            # print(protocol['NCT_ID'])\n",
    "            data = response.json()\n",
    "            data = data['protocolSection']\n",
    "            if \"officialTitle\" in data[\"identificationModule\"]:\n",
    "                protocol['TITLE'] = data[\"identificationModule\"][\"officialTitle\"]\n",
    "            # print(data[\"identificationModule\"])\n",
    "\n",
    "            if \"organization\" in data['identificationModule']:\n",
    "                # print(data[\"identificationModule\"][\"organization\"])\n",
    "                protocol[\"ORGANIZATION\"] = str(data[\"identificationModule\"][\"organization\"])\n",
    "                # print(protocol[\"ORGANIZATION\"])\n",
    "            protocol['SHORT_TITLE'] = data[\"identificationModule\"][\"briefTitle\"]\n",
    "            protocol['SPONSOR'] = data['sponsorCollaboratorsModule'][\"leadSponsor\"][\"name\"]\n",
    "            protocol['DETAILED_ELIGIBILITY'] = data[\"eligibilityModule\"][\"eligibilityCriteria\"]\n",
    "            if \"detailedDescription\" in data['descriptionModule']:\n",
    "                protocol[\"DESCRIPTION\"] = data['descriptionModule'][\"detailedDescription\"]\n",
    "            protocol[\"SUMMARY\"] = data[\"descriptionModule\"][\"briefSummary\"]\n",
    "            protocol[\"STATUS\"] = data['statusModule']['overallStatus']\n",
    "            if \"outcomesModule\" in data:\n",
    "                if \"primaryOutcomes\" in data[\"outcomesModule\"]:\n",
    "                    primary_outcomes = str(data[\"outcomesModule\"][\"primaryOutcomes\"])\n",
    "                    protocol[\"PRIMARY_OUTCOMES\"] = primary_outcomes\n",
    "\n",
    "                if \"secondaryOutcomes\" in data[\"outcomesModule\"]:\n",
    "                    secondary_outcomes = str(data[\"outcomesModule\"][\"secondaryOutcomes\"])\n",
    "                    protocol[\"SECONDARY_OUTCOMES\"] = secondary_outcomes\n",
    "\n",
    "                if \"otherOutcomes\" in data[\"outcomesModule\"]:\n",
    "                    other_outcomes = str(data[\"outcomesModule\"][\"otherOutcomes\"])\n",
    "                    protocol[\"OTHER_OUTCOMES\"] = other_outcomes\n",
    "                \n",
    "            if isinstance(data[\"eligibilityModule\"][\"stdAges\"],list):\n",
    "                text = \"\"\n",
    "                for std in data[\"eligibilityModule\"][\"stdAges\"]:\n",
    "                    text += std +', '\n",
    "                text = text.rstrip(', ')\n",
    "                protocol[\"AGE_DESCRIPTION\"] = text\n",
    "            else:\n",
    "                protocol[\"AGE_DESCRIPTION\"] = data[\"eligibilityModule\"][\"stdAges\"]\n",
    "\n",
    "            if \"conditionsModule\" in data:\n",
    "                protocol['CONDITIONS'] = []\n",
    "                if \"conditions\" in data[\"conditionsModule\"]:\n",
    "                    for condition in data[\"conditionsModule\"][\"conditions\"]:\n",
    "                        # print(condition)\n",
    "                        protocol['CONDITIONS'].append(condition)\n",
    "\n",
    "                if \"keywords\" in data[\"conditionsModule\"]:\n",
    "                    for keyword in data[\"conditionsModule\"][\"keywords\"]:\n",
    "                        # print(keyword)\n",
    "                        protocol[\"CONDITIONS\"].append(keyword)\n",
    "                \n",
    "            if \"contactsLocationsModule\" in data:\n",
    "\n",
    "                contacts_locations_module = data[\"contactsLocationsModule\"]\n",
    "                protocol[\"OVERALL_OFFICIALS\"] = \"\"  # Initialize as empty string\n",
    "                \n",
    "                if \"overallOfficials\" in contacts_locations_module:\n",
    "                    officials = contacts_locations_module[\"overallOfficials\"]\n",
    "                    protocol[\"OVERALL_OFFICIALS\"] = str(officials) # Remove trailing \"; \"\n",
    "\n",
    "                if  protocol[\"OVERALL_OFFICIALS\"] == \"\":\n",
    "                    del protocol[\"OVERALL_OFFICIALS\"]\n",
    "                protocol[\"LOCATIONS\"] = []  # Initialize as empty string\n",
    "                \n",
    "                if \"locations\" in contacts_locations_module:\n",
    "                    locations = contacts_locations_module[\"locations\"]\n",
    "                    location_dict = {}\n",
    "                    for loc in locations[:15]:\n",
    "                        if \"facility\" in loc:\n",
    "                            location_dict[\"facility\"] = loc[\"facility\"]\n",
    "                        if \"city\" in loc:\n",
    "                            location_dict[\"city\"] = loc[\"city\"]\n",
    "                        if \"state\" in loc:\n",
    "                            location_dict[\"state\"] = loc[\"state\"]\n",
    "                        if \"country\" in loc:\n",
    "                            location_dict[\"country\"] = loc[\"country\"]\n",
    "\n",
    "                        protocol[\"LOCATIONS\"].append(location_dict)\n",
    "\n",
    "                protocol[\"LOCATIONS\"] = str(protocol[\"LOCATIONS\"])\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for NCT ID {nct_id}: {str(e)}\")\n",
    "        \n",
    "    trials_info.append(protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"https://clinicaltrials.gov/api/v2\"\n",
    "trials_info = []\n",
    "nct_id = \"NCT05130268\"\n",
    "url = f\"{base_url}/studies/{nct_id}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../database/protocolDB/ucspecific-1.json\", \"w\") as json_file:\n",
    "        json.dump(trials_info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in data[\"protocolSection\"]['contactsLocationsModule'][\"locations\"]:\n",
    "    if ele['facility'] == \"University of California - Irvine\":\n",
    "        print(ele[\"contacts\"][0]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = {}\n",
    "protocol['NCT_ID'] = nct_id    \n",
    "data = response.json()\n",
    "# title extraction:\n",
    "data = data['protocolSection']\n",
    "protocol['TITLE'] = data[\"identificationModule\"][\"officialTitle\"]\n",
    "protocol['SHORT_TITLE'] = data[\"identificationModule\"][\"briefTitle\"]\n",
    "protocol['SPONSOR'] = data['sponsorCollaboratorsModule'][\"leadSponsor\"][\"name\"]\n",
    "protocol['DETAILED_ELIGIBILITY'] = data[\"eligibilityModule\"][\"eligibilityCriteria\"]\n",
    "if \"detailedDescription\" in data['descriptionModule']:\n",
    "    protocol[\"DESCRIPTION\"] = data['descriptionModule'][\"detailedDescription\"]\n",
    "protocol[\"SUMMARY\"] = data[\"descriptionModule\"][\"briefSummary\"]\n",
    "protocol[\"STATUS\"] = data['statusModule']['overallStatus']\n",
    "protocol[\"OUTCOME_DESCRIPTION\"] = data[\"outcomesModule\"][\"primaryOutcomes\"][0][\"description\"]\n",
    "protocol[\"OUTCOME_MEASURE\"] = data[\"outcomesModule\"][\"primaryOutcomes\"][0][\"measure\"]\n",
    "protocol[\"OUTCOME_TIMEFRAME\"] = data[\"outcomesModule\"][\"primaryOutcomes\"][0][\"timeFrame\"]\n",
    "# protocol[\"AGE_DESCRIPTION\"] = data[\"eligibilityModule\"][\"stdAges\"]\n",
    "text = \"\"\n",
    "for std in data[\"eligibilityModule\"][\"stdAges\"]:\n",
    "    text += std +', '\n",
    "text = text.rstrip(', ')\n",
    "protocol[\"AGE_DESCRIPTION\"] = text\n",
    "\n",
    "for ele in data['contactsLocationsModule'][\"locations\"]:\n",
    "    if ele['facility'] == \"University of California - Irvine\":\n",
    "        protocol[\"INVESTIGATOR_NAME\"] = ele[\"contacts\"][0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../database/protocolDB/moreProtocols.json\", \"w\") as json_file:\n",
    "        json.dump(trials_info, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"The trial identifier is NCT05645744 and NCT12345678 is another trial.\"\n",
    "\n",
    "pattern = r'\\bNCT\\d{8}\\b'\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(matches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(matches)>0:\n",
    "\n",
    "    matches_text = \", \".join(matches)\n",
    "    results = db.similarity_search(matches_text, k=3)\n",
    "    \n",
    "else:\n",
    "    results = db.similarity_search(question, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History Aware Retriever Trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install scipy \n",
    "!pip3 install plotly\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databutton as db\n",
    "import streamlit as st\n",
    "import time \n",
    "import openai\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from brain import custom_search\n",
    "import os\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"openai_key.txt\", 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_path = r\"../database/vectorDB/UCDB\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "db = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U langchain langchain-community \n",
    "!pip3 install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-4o', temperature=0.1, api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"can you suggest trials related to brain tumor?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = history_aware_retriever.invoke({\"chat_history\": [],\"input\":input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'NCT00030628'}, page_content='NCT00030628 A Phase III Randomized Trial Of The Role Of Whole Brain Radiation Therapy In Addition To Radiosurgery In The Management Of Patients With One To Three Cerebral Metastases Radiosurgery With or Without Whole-Brain Radiation Therapy in Treating Patients With Brain Metastases Alliance for Clinical Trials in Oncology {\\'fullName\\': \\'Alliance for Clinical Trials in Oncology\\', \\'class\\': \\'OTHER\\'} Inclusion Criteria:\\n\\n* Diagnosis of cerebral metastases meeting all of the following requirements:\\n\\n  * 1-3 de novo lesions\\n  * Metastases must be from a histologically confirmed extracerebral primary site, another metastatic site, or from the metastatic brain lesion(s)\\n  * Each lesion must be less than 3.0 cm by contrasted MRI of the brain\\n  * Lesions must not be within 5 mm of optic chiasm or within the brainstem\\n* No primary germ cell tumor, small cell carcinoma, or lymphoma\\n* No leptomeningeal metastases\\n* Eligible for treatment with gamma knife or linear accelerator-based radiosurgery\\n* Performance status - ECOG 0-2\\n* Performance status - Zubrod 0-2\\n* Not pregnant\\n* Negative pregnancy test\\n* Fertile patients must use effective contraception\\n\\n  \\\\* Male patients must continue to use contraception for 3 months after the completion of radiotherapy\\n* No pacemaker or other MRI-incompatible metal in body\\n* No known allergy to gadolinium\\n* Deemed to be at low risk for recurrence from any prior malignancies\\n* At least 7 days since prior chemotherapy\\n* Concurrent hormonal agents allowed\\n* Concurrent steroids allowed\\n* No prior cranial radiotherapy\\n* No prior resection of cerebral metastasis\\n* Concurrent anticonvulsants allowed provided therapeutic serum/plasma level maintained before study intervention OBJECTIVES:\\n\\n* Compare the overall survival of patients with 1 to 3 cerebral metastases treated with radiosurgery with or without whole brain radiotherapy.\\n* Compare the time to CNS failure (brain) in patients treated with these regimens.\\n* Compare the quality of life, duration of functional independence, and long-term neurocognitive status in patients treated with these regimens.\\n* Compare the post-treatment toxic effects of these regimens in these patients.\\n\\nOUTLINE: This is a randomized, multicenter study. Patients are stratified according to age (60 and over vs under 60), extracranial disease (controlled for more than 3 months vs controlled for 3 months or less), and number of brain metastases (1 vs more than 1). Patients are randomized to 1 of 2 treatment arms.\\n\\n* Arm I: Patients undergo radiosurgery.\\n* Arm II: Patients undergo radiosurgery. Within 14 days, patients then undergo whole brain radiotherapy 5 days a week for 2.5 weeks.\\n\\nQuality of life is assessed at baseline, the beginning of each treatment, at week 6, every 3 months for 1 year, every 4 months for 1 year, and then every 6 months for 2 years.\\n\\nPatients are followed at weeks 6 and 12, every 3 months for 9 months, every 4 months for 1 year, every 6 months for 2 years, and then annually thereafter.\\n\\nPROJECTED ACCRUAL: A total of 480 patients (240 per treatment arm) will be accrued for this study within 5 years. RATIONALE: Radiation therapy uses high-energy x-rays to damage tumor cells. Radiosurgery may be able to deliver x-rays directly to the tumor and cause less damage to normal tissue. It is not yet known if radiosurgery is more effective with or without whole-brain radiation therapy in treating brain metastases.\\n\\nPURPOSE: Randomized phase III trial to compare the effectiveness of radiosurgery with or without whole-brain radiation therapy in treating patients who have brain metastases. COMPLETED [{\\'measure\\': \\'Overall survival (OS)\\', \\'timeFrame\\': \\'Up to 6 months\\'}] [{\\'measure\\': \\'Time to CNS failure\\', \\'timeFrame\\': \\'Up to 4 years\\'}, {\\'measure\\': \\'Change in QOL between SRS and SRS + WBRT treatment groups using the FACT-BR questionnaire\\', \\'timeFrame\\': \\'From baseline to up to 3 months\\'}, {\\'measure\\': \\'Change in the duration of functional independence using the Barthel ADL Index score\\', \\'timeFrame\\': \\'From baseline to up to 4 years\\'}] ADULT, OLDER_ADULT [\\'Metastatic Cancer\\', \\'tumors metastatic to brain\\'] [{\\'name\\': \\'Anthony Asher, MD, FACS\\', \\'affiliation\\': \\'Carolina Neurosurgery and Spine Associates\\', \\'role\\': \\'STUDY_CHAIR\\'}] [{\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \"Brigham and Women\\'s Hospital\", \\'city\\': \\'Boston\\', \\'state\\': \\'Massachusetts\\', \\'country\\': \\'United States\\'}] '),\n",
       " Document(metadata={'source': 'NCT05576103'}, page_content=\"NCT05576103 Longitudinal Prospective Study of Neurocognitive Outcomes and Multimodal Quantitative Neuroimaging Outcomes in Primary Brain Tumor Patients Receiving Brain Radiotherapy Longitudinal Prospective Study of Neurocognition & Neuroimaging in Primary BT Patients Jona Hattangadi-Gluth {'fullName': 'University of California, San Diego', 'class': 'OTHER'} Inclusion Criteria:\\n\\n1. Patients 18 years or older\\n2. Karnofsky performance status (KPS) 70\\n3. Life expectancy of 1 year\\n4. Primary brain tumor patients who will receive fractionated partial brain RT\\n5. Able to complete neurocognitive assessments\\n\\nExclusion Criteria:\\n\\n1. Inability to undergo MRI with contrast\\n2. Prior brain RT Background: Fractionated radiation therapy (RT) is a mainstay in the treatment of primary and metastatic brain tumors. However, RT to the brain is associated with an inevitable decline in neurocognitive function in up to 90% of patients who survive more than 6 months after irradiation. Radiation to the brain results in an inevitable decline in neurocognitive function, mediated by tissue injury to white matter, cortex and subcortical areas. With quantitative magnetic resonance imaging (MRI) techniques, investigators can directly and non-invasively measure such changes.\\n\\nObjective/Hypothesis:\\n\\nThe purpose of this study is to examine radiation-induced imaging changes in normal brain tissue over time in primary brain tumor patients, and correlate these with neurocognitive outcomes. The overarching goal is to better identify sensitive brain regions so that future radiation techniques can be optimally designed to mitigate collateral damage.\\n\\nSpecific Aims:\\n\\n1. To identify microstructural changes in subcortical white matter, hippocampus, and cortex associated with quantified regional exposure to fractionated brain radiotherapy using advanced quantitative neuroimaging imaging\\n2. To identify changes in neurocognitive functioning in primary brain tumor patients after brain radiotherapy\\n\\nStudy Design:\\n\\nThe investigators will prospectively enroll primary brain tumor patients undergoing fractionated partial brain radiation therapy. Patients will undergo volumetric and diffusion brain MRI (per clinical standard-of-care) and a neurocognitive battery of tests at baseline (pre-treatment), 3 months, 6 months, and 12 months post-treatment. Clinical data including age, gender, educational status, tumor size and histology, steroid use, antiepileptic drug use and chemotherapy will be recorded. In this proposal, the investigators introduce a novel, translational study to prospectively examine primary brain tumor patients undergoing fractionated radiation therapy to the brain. Quantitative neuroimaging, radiation dose information, and directed neurocognitive testing will be acquired through this study to improve understanding of cognitive changes associated with radiation dosage to non-targeted tissue, and will provide the basis for evidence-based cognitive- sparing brain radiotherapy. RECRUITING [{'measure': 'Longitudinal changes in imaging biomarker volume (cc) from volumetric MR imaging', 'description': 'To measure longitudinal changes in volume (cc) from volumetric MR imaging', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Longitudinal changes in imaging biomarker mean diffusivity (MD) in white matter from DTI imaging', 'description': 'To measure longitudinal changes in MD (mm squared/second) from DTI imaging', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Longitudinal changes in imaging biomarker fractional anisotropy (FA) in white matter from DTI imaging', 'description': 'To measure longitudinal changes in FA (unitless index between 0 and 1) from DTI imaging', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Change in Memory after RT', 'description': 'To evaluate the change from baseline to post-RT verbal memory performance when performing fractionated partial brain RT. Verbal memory outcomes and measurements include: Hopkins Verbal Learning Test-Revised (HVLT-R)-Immediate, Delayed Recall. Brief Visuospatial Memory Test-Revised (BVMT-R)- Total, Delayed Recall\\\\n\\\\nScale of scores is:\\\\n\\\\nHopkins Verbal Learning Test-Revised (HVLT-R)-Immediate, Delayed Recall: 0-36 for Immediate, 0-12 for Delayed. For both tests, higher scores indicate better performance.\\\\n\\\\nBrief Visuospatial Memory Test-Revised (BVMT-R)- Total, Delayed Recall: l: 0-36 for Immediate, 0-12 for Delayed.\\\\n\\\\nFor both tests, higher scores indicate better performance.', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Change in Executive Functioning after RT', 'description': 'To evaluate the change from baseline to post-RT executive functioning performance when performing fractionated partial brain RT. Executive functioning outcomes and measurements include: Controlled Oral Word Association Test (COWA): letter fluency, Trail Making Test Part B (TMT-B).\\\\n\\\\nScale of scores is:\\\\n\\\\nControlled Oral Word Association Test (COWA): letter fluency: 0- no upper limit. Higher score indicates better performance Trail Making Test Part B (TMT-B): 0-240. Higher score indicates poorer performance', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Change in Attention/Processing Speed after RT', 'description': 'To evaluate the change from baseline to post-RT Attention/Processing Speed performance when performing fractionated partial brain RT. Attention/Processing Speed outcomes and measurements include: Trail Making Test Part A (TMT-A)\\\\n\\\\nScale of scores is:\\\\n\\\\nTrail Making Test Part A (TMT-A): 0-240. Higher score indicates poorer performance.', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Change in Language functioning after RT', 'description': 'To evaluate the change from baseline to post-RT Language performance when performing fractionated partial brain RT in patients with primary brain tumor. Language outcomes and measurements include: Boston Naming Test (BNT), Controlled Oral Word Association Test (COWA): category fluency\\\\n\\\\nScale of scores is:\\\\n\\\\nBoston Naming Test (BNT): 0-60 Controlled Oral Word Association Test (COWA): category fluency: 0-no upper limit.\\\\n\\\\nFor both tests, higher score indicates better performance.', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Change in Fine Motor Skills after RT', 'description': 'To evaluate the change from baseline to post-RT Fine Motor Skills performance when performing fractionated partial brain RT in patients with primary brain tumor. Fine Motor Skills outcomes and measurements include:\\\\n\\\\nTrail Making Test Motor Speed; Grooved Pegboard Test', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}, {'measure': 'Change in health-related quality of life (hrQoL) from baseline to 5 years after RT', 'description': 'To evaluate the change from baseline to post-RT health-related quality of life (hrQoL) when performing fractionated partial brain RT in patients with primary brain tumor. Quality of life outcomes and measurements include: Beck Depression inventory II (BDI II), Beck Anxiety Inventory (BAI) and FACT-BR (Functional Assessment of Cancer Therapy - Brain).\\\\n\\\\nFACT-BR (Functional Assessment of Cancer Therapy - Brain) higher scores on each subscale indicate greater hrQoL.', 'timeFrame': 'baseline (pre-treatment), 3 months, 6 months, 12 months post-treatment'}] ADULT, OLDER_ADULT ['Primary Brain Tumor', 'Glioma', 'Meningioma', 'Schwannoma'] [{'facility': 'Moores Cancer Center', 'city': 'San Diego', 'state': 'California', 'country': 'United States'}] \"),\n",
       " Document(metadata={'source': 'NCT03550391'}, page_content='NCT03550391 A Phase III Trial of Stereotactic Radiosurgery Compared With Hippocampal-Avoidant Whole Brain Radiotherapy (HA-WBRT) Plus Memantine for 5 or More Brain Metastases Stereotactic Radiosurgery Compared With Hippocampal-Avoidant Whole Brain Radiotherapy (HA-WBRT) Plus Memantine for 5 or More Brain Metastases Canadian Cancer Trials Group {\\'fullName\\': \\'Canadian Cancer Trials Group\\', \\'class\\': \\'NETWORK\\'} Inclusion Criteria:\\n\\n* Patients must have 5 or more brain metastases as counted on a T1 contrast enhanced MRI obtained  30 days from randomization (maximum 15 brain metastases).\\n* Patients must have a pathological diagnosis (cytological or histological) of a non-hematopoietic malignancy.\\n* The largest brain metastasis must measure \\\\<2.5 cm in maximal diameter.\\n* Centre must have the ability to treat patients with either a Gamma Knife, Cyberknife, or a linear accelerator-based radiosurgery system.\\n* Patient must be \\\\> 18 years of age.\\n* Patient is able (i.e. sufficiently fluent) and willing to complete the quality of life questionnaires in either English or French either alone or with assistance.\\n* ECOG performance status 0, 1, or 2.\\n* Creatinine clearance must be  30 ml/min within 28 days prior to registration.\\n* The Neurocognitive Testing examiner must have credentialing confirming completion of the neurocognitive testing training.\\n* Facility is credentialed by IROC to perform SRS and HA-WBRT. The treating centre must have completed stereotactic radiosurgery credentialing of the specific system(s) to be used in study patients. The treating centre must have completed IMRT credintialing of this specific IMRT systems to be used in study patients for the purposes of HA-WBRT.\\n* Patient consent must be appropriately obtained in accordance with applicable local and regulatory requirements. Each patient must sign a consent form prior to enrolment in the trial to document their willingness to participate.\\n* A similar process must be followed for sites outside of Canada as per their respective cooperative group\\'s procedures.\\n* Patients must be accessible for treatment and follow-up. Investigators must assure themselves the patients randomized on this trial will be available for complete documentation of the treatment, adverse events, and follow-up.\\n* In accordance with CCTG policy, protocol treatment is to begin within 14 days of patient enrolment.\\n* Women/men of childbearing potential must have agreed to use a highly effective contraceptive method.\\n\\nExclusion Criteria:\\n\\n* Pregnant or nursing women.\\n* Men or women of childbearing potential who are unwilling to employ adequate contraception.\\n* Inability to complete a brain MRI.\\n* Known allergy to gadolinium.\\n* Prior cranial radiation therapy.\\n* Planned cytotoxic chemotherapy within 48 hours prior or after the SRS or HA-WBRT.\\n* Primary germ cell tumour, small cell carcinoma, or lymphoma.\\n* Widespread definitive leptomeningeal metastasis. This includes cranial nerve palsy, leptomeningeal carcinomatosis, ependymal involvement, cranial nerve involvement on imaging, suspicious linear meningeal enhancement, or cerebrospinal fluid (CSF) positive for tumour cells.\\n* A brain metastasis that is located  5 mm of the optic chiasm or either optic nerve.\\n* Surgical resection of a brain metastasis (stereotactic biopsies will be allowed).\\n* More than 15 brain metastases on a volumetric T1 contrast MRI (voxels of 1mm or smaller) performed within the past 14 days, or more than 10 metastases in the case of a non-volumetric MRI.\\n* Prior allergic reaction to memantine.\\n* Current alcohol or drug abuse.\\n* Current use of NMDA antagonists, such as amantadine, ketamine, or dextromethorphan.\\n* Diagnosis of chronic liver disease/cirrhosis of the liver (e.g. Child-Pugh class B or C).\\n* Patients with architectural distortion of lateral ventricular systems, which, in the opinion of the local investigator, makes hippocampal delineation challenging The purpose of this research study is to compare the effects (good or bad) of receiving stereotactic radiosurgery (SRS) versus receiving hippocampal-avoidant whole brain radiotherapy (HA-WBRT) plus a drug called memantine, on brain metastases. Receiving SRS could control cancer that has spread to the brain.\\n\\nThis study will allow the researchers to know whether this different approach is better, the same, or worse than the usual approach. To decide if it is better, the study doctors will be looking to see if the stereotactic radiosurgery (SRS) helps to either slow the growth of cancer or stop it from coming back, compared to the usual approach. Doctors will also look to see if this new approach increases the life span of patients with this type of cancer, and if it helps with quality of life and cancer related symptoms.\\n\\nThe usual approach for patients who are not in a study is treatment with whole brain radiation therapy alone (WBRT). Stereotactic radiosurgery (SRS) is a commonly used treatment for brain tumors. It is a one-day (or in some cases two day), out-patient procedure during which a high dose of radiation is delivered to small spots in the brain while excluding the surrounding normal brain.\\n\\nWhole brain radiation therapy with hippocampal avoidance (HA-WBRT) is when radiation therapy is given to the whole brain, while trying to decrease the amount of radiation that is delivered to the area of the hippocampus. The hippocampus is a brain structure that is important for memory. Memantine is a drug that is given to help relieve symptoms that can be caused by WBRT, including problems with memory and other mental symptoms.\\n\\nHealth Canada, the regulatory body that oversees the use of drugs in Canada, has not approved the sale or use of memantine in combination with WBRT to treat this kind of cancer, although they have allowed its use in this study. RECRUITING [{\\'measure\\': \\'Overall Survival\\', \\'description\\': \\'To compare the overall survival in patients with five to fifteen brain metastases who receive SRS compared to patients who receive HA-WBRT + memantine\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Neurocognitive progression-free survival\\', \\'description\\': \\'To compare the neurocognitive progression-free survival in patients with five to fifteen brain metastases who receive SRS compared to patients who receive HA-WBRT + memantine\\', \\'timeFrame\\': \\'4.5 years\\'}] [{\\'measure\\': \\'Time to central nervous system (CNS) failure (local, distant, and leptomeningeal) in patients who receive SRS compared to patients who receive HA-WBRT + memantine\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Difference in CNS failure patterns (local, distant, or leptomeningeal) in patients who receive SRS compared to patients who receive HA-WBRT + memantine\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Number of salvage procedures following SRS in comparison to HA-WBRT + memantine\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Neurocognitive progression-free survival in patients who receive SRS compared to HA-WBRT + memantine\\', \\'description\\': \\'measured from date the patient is randomized to date at which there is a drop of at least 1.5 standard deviations from baseline in two of the six neurocognitive tests (all tests are standardized based on published norms)\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Tabulate and descriptively compare the post-treatment adverse events associated with the interventions.\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Time delay to (re-)initiation of systemic therapy in patients receiving SRS in comparison to HA-WBRT + memantine\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Prospectively validate a predictive nomogram for distant brain failure in patients who receive SRS\\', \\'description\\': \\'a predictive nomogram as a clinically useful tool to determine the likelihood of distant brain failure (DBF) at different time points after radiosurgery\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Compare the estimated cost of brain-related therapies in patients who receive SRS compared to patients who receive HA-WBRT + memantine.\\', \\'description\\': \\'Comparison based on payer rates (Medicare for US / provincial heath authorities in Canadian jurisdictions with activity-based funding)\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Quality of life, as assessed by the European Organization for Research and Treatment of Cancer Quality of Life Questionnaire (QLQ-C30) with brain cancer module (BN20)\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Quality of life assessed by ECOG performance status\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Quality of life, as assessed by EQ-5D-5L\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Collect plasma to evaluate whether detectable somatic mutations in liquid biopsy can enhance prediction of the overall survival and development of new brain metastases.\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Analysis of serum samples for inflammatory biomarker C-reactive protein and brain-derived-neurotrophic factor (BDNF) to elucidate molecular/genomic mechanisms of neurocognitive decline and associated radiographic changes\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Collect whole-brain dosimetry in SRS patients to be prospectively correlated with cognitive toxicity, intracranial control and radiation necrosis\\', \\'timeFrame\\': \\'4.5 years\\'}, {\\'measure\\': \\'Evaluate serial changes in imaging features found in routine MRI images (T2w changes, morphometry) that may predict tumour control and/or neurocognitive outcomes\\', \\'timeFrame\\': \\'4.5 years\\'}] ADULT, OLDER_ADULT [\\'Brain Metastases\\'] [{\\'name\\': \\'David Roberge\\', \\'affiliation\\': \"CHUM-Centre Hospitalier de l\\'Universite de Montreal\", \\'role\\': \\'STUDY_CHAIR\\'}, {\\'name\\': \\'Michael Chan\\', \\'affiliation\\': \\'Wake Forest School of Medicine, Winston-Salem, NC\\', \\'role\\': \\'STUDY_CHAIR\\'}, {\\'name\\': \\'Vina Gondi\\', \\'affiliation\\': \\'Northwestern Medicine Cancer Center, Warrenville IL\\', \\'role\\': \\'STUDY_CHAIR\\'}] [{\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Kaiser Permanente Cancer Treatment Center\\', \\'city\\': \\'South San Francisco\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}] '),\n",
       " Document(metadata={'source': 'NCT02179086'}, page_content='NCT02179086 Randomized Phase II Trial of Hypofractionated Dose-Escalated Photon IMRT or Proton Beam Therapy Versus Conventional Photon Irradiation With Concomitant and Adjuvant Temozolomide in Patients With Newly Diagnosed Glioblastoma Dose-Escalated Photon IMRT or Proton Beam Radiation Therapy Versus Standard-Dose Radiation Therapy and Temozolomide in Treating Patients With Newly Diagnosed Glioblastoma NRG Oncology {\\'fullName\\': \\'NRG Oncology\\', \\'class\\': \\'OTHER\\'} Inclusion Criteria:\\n\\n* PRIOR TO STEP 1 REGISTRATION\\n* A diagnostic contrast-enhanced magnetic resonance imaging (MRI) (no other scan type allowed) of the brain must be performed postoperatively within 72 hours of resection; the enhancing tumor must have a maximal diameter of 5 cm; the tumor diameter will be the greatest diameter as measured on the contrast-enhanced postoperative MRI and will include residual disease and/or the postoperative surgical cavity as appropriate; for cases where residual disease or postoperative surgical cavity is NOT identifiable (e.g., polar glioblastomas \\\\[GBMs\\\\] where a polar lobectomy is performed), the patient will be excluded from the trial\\n* The GBM tumor must be located in the supratentorial compartment only (any component involving the brain stem or cerebellum is not allowed)\\n* Patients must provide study-specific informed consent prior to step 1 registration\\n* PRIOR TO STEP 2 REGISTRATION\\n* Histologically proven diagnosis of glioblastoma (World Health Organization \\\\[WHO\\\\] grade IV) confirmed by central review prior to step 2 registration\\n* Tumor tissue that is determined by central pathology review prior to step 2 registration to be of sufficient quantity for analysis of O(6)-methylguanine-DNA-methyltransferase (MGMT) status\\n\\n  * Patients must have at least 1 block of tumor tissue; submission of 2 blocks is strongly encouraged to maximize the chances of eligibility; at least 1 cubic centimeter of tissue composed primarily of tumor must be present\\n  * Diagnosis must be made by surgical excision, either partial or complete; stereotactic biopsy or cavitron ultrasonic suction aspirator (CUSA) technique are not allowed\\n* History/physical examination within 28 days prior to step 2 registration\\n* The patient must have recovered from effects of surgery, postoperative infection, and other complications within 28 days prior to step 2 registration\\n* Documentation of steroid doses within 28 days prior to step 2 registration\\n* Karnofsky performance status \\\\>= 70 within 28 days prior to step 2 registration\\n* Age \\\\>= 18\\n* Absolute neutrophil count (ANC) \\\\>= 1,800 cells/mm\\\\^3\\n* Platelets \\\\>= 100,000 cells/mm\\\\^3\\n* Hemoglobin \\\\>= 10.0 g/dl (note: the use of transfusion or other intervention to achieve hemoglobin (Hgb) \\\\>= 10.0 g/dl is acceptable)\\n* Bilirubin =\\\\< 1.5 upper limit of normal (ULN)\\n* Alanine aminotransferase (ALT) and aspartate aminotransferase (AST) =\\\\< 3 x ULN\\n* Negative serum pregnancy test obtained for females of child-bearing potential within 28 days prior to step 2 registration\\n\\nExclusion Criteria:\\n\\n* Prior invasive malignancy (except non-melanomatous skin cancer) unless disease-free for a minimum of 3 years; (for example, carcinoma in situ of the breast, oral cavity, or cervix are all permissible)\\n* Recurrent or multifocal malignant gliomas\\n* Any site of distant disease (for example, drop metastases from the GBM tumor site)\\n* Prior chemotherapy or radiosensitizers for cancers of the head and neck region; note that prior chemotherapy for a different cancer is allowable (except temozolomide)\\n* Prior use of Gliadel wafers or any other intratumoral or intracavitary treatment are not permitted\\n* Prior radiotherapy to the head or neck (except for T1 glottic cancer), resulting in overlap of radiation fields\\n* Severe, active co-morbidity, defined as follows:\\n\\n  * Unstable angina at step 2 registration\\n  * Transmural myocardial infarction within the last 6 months prior to step 2 registration\\n  * Evidence of recent myocardial infarction or ischemia by the findings of S-T elevations of \\\\>= 2 mm using the analysis of an electrocardiogram (EKG) performed within 28 days prior to step 2 registration\\n  * New York Heart Association grade II or greater congestive heart failure requiring hospitalization within 12 months prior to step 2 registration\\n  * Serious and inadequately controlled arrhythmia at step 2 registration\\n  * Serious or non-healing wound, ulcer or bone fracture or history of abdominal fistula, intra-abdominal abscess requiring major surgical procedure, open biopsy or significant traumatic injury within 28 days prior to step 2 registration, with the exception of the craniotomy for surgical resection\\n  * Acute bacterial or fungal infection requiring intravenous antibiotics at the time of step 2 registration\\n  * Hepatic insufficiency resulting in clinical jaundice and/or coagulation defects; note, however, that laboratory tests for coagulation parameters are not required for entry into this protocol\\n  * Chronic obstructive pulmonary disease exacerbation or other respiratory illness requiring hospitalization or precluding study therapy at the time of step 2 registration\\n  * Acquired immune deficiency syndrome (AIDS) based upon current Centers for Disease Control and Prevention (CDC) definition; note, however, that human immunodeficiency virus (HIV) testing is not required for entry into this protocol\\n  * Any other severe immunocompromised condition\\n  * Active connective tissue disorders, such as lupus or scleroderma, that in the opinion of the treating physician may put the patient at high risk for radiation toxicity\\n  * End-stage renal disease (ie, on dialysis or dialysis has been recommended)\\n  * Any other major medical illnesses or psychiatric treatments that in the investigator\\'s opinion will prevent administration or completion of protocol therapy\\n* Pregnancy or women of childbearing potential and men who are sexually active and not willing/able to use medically acceptable forms of contraception\\n* Patents treated on any other therapeutic clinical protocols within 30 days prior to step 2 registration\\n* Inability to undergo MRI (e.g., due to safety reasons, such as presence of a pacemaker, or severe claustrophobia)\\n* Postoperative tumor plus surgical bed size exceeds 5 cm in maximum diameter. PRIMARY OBJECTIVES:\\n\\nI. To determine if dose-escalated and -intensified photon IMRT or proton beam therapy (using a dose-per-fraction escalation with simultaneous integrated boost) with concomitant and adjuvant temozolomide improves overall survival, as compared to standard-dose photon irradiation with concomitant and adjuvant temozolomide.\\n\\nSECONDARY OBJECTIVES:\\n\\nI. To indirectly compare dose-escalated and -intensified photon IMRT to dose-escalated and -intensified proton beam therapy in terms of overall survival.\\n\\nII. To indirectly compare and record toxicities of dose-escalated and -intensified photon IMRT versus dose-escalated and -intensified proton beam therapy and directly compare the toxicities of these approaches versus standard-dose photon irradiation on the backbone of concomitant and adjuvant temozolomide.\\n\\nIII. To determine if dose-escalated and -intensified photon IMRT or proton beam therapy (using a dose-per-fraction escalation with simultaneous integrated boost) with concomitant and adjuvant temozolomide improves perceived cognitive symptom severity, as compared to standard-dose photon irradiation with concomitant and adjuvant temozolomide.\\n\\nIV. To determine if dose-escalated and -intensified photon IMRT or proton beam therapy (using a dose-per-fraction escalation with simultaneous integrated boost) with concomitant and adjuvant temozolomide improves neurocognitive function, as compared to standard-dose photon irradiation with concomitant and adjuvant temozolomide.\\n\\nV. To indirectly determine if dose-escalated and -intensified proton beam therapy with concomitant and adjuvant temozolomide improves perceived cognitive symptom severity, as compared to dose-escalated and -intensified photon IMRT, and to directly compare symptom burden with these approaches versus standard-dose photon irradiation on the backbone of concomitant and adjuvant temozolomide.\\n\\nVI. To indirectly determine if dose-escalated and -intensified proton beam therapy with concomitant and adjuvant temozolomide improves neurocognitive function, as compared to dose-escalated and -intensified photon IMRT, and to directly compare neurocognitive function with these approaches versus standard-dose photon irradiation on the backbone of concomitant and adjuvant temozolomide.\\n\\nTERTIARY OBJECTIVES:\\n\\nI. Tissue banking for future translational science projects that will be determined based on the state of the science at the time the primary endpoint is reported and will be submitted to National Cancer Institute (NCI) for review and approval.\\n\\nII. To prospectively compare CD4 lymphopenia between dose-escalated and intensified proton beam therapy, dose-escalated and -intensified photon IMRT, and standard-dose photon irradiation and determine whether CD4 lymphopenia impacts overall survival.\\n\\nIII. To explore the most appropriate and clinically relevant technological parameters to ensure quality and effectiveness throughout radiation therapy processes, including imaging, simulation, patient immobilization, target and critical structure definition, treatment planning, image guidance and delivery.\\n\\n* To establish feasibility and clinical relevancy of quality assurance guidelines.\\n* To evaluate efficacy of quality assurance tools.\\n\\nIV. To explore the most appropriate and clinically relevant advanced and standard MRI imaging parameters.\\n\\n* To evaluate the feasibility of differentiating pseudo-progression and true progression in a multi institutional setting using MR diffusion and perfusion imaging.\\n* To evaluate for early, imaging biomarkers of response and overall survival.\\n\\nOUTLINE: Patients are assigned to 1 of 2 groups depending on enrolling institution. Within each group, patients will be randomized 1:2 in favor of the experimental arms.\\n\\nGROUP I (PHOTON IMRT CENTERS): Patients are randomized to 1 of 2 treatment arms.\\n\\nARM A1: Patients undergo standard-dose photon irradiation using 3-dimensional conformal radiation therapy (3D-CRT) or IMRT once daily (QD), 5 days a week for 23 fractions plus a boost of 7 additional fractions.\\n\\nARM B: Patients undergo dose-escalated and -intensified photon IMRT QD, 5 days a week for a total of 30 fractions.\\n\\nGROUP II (PROTON CENTERS): Patients are randomized to 1 of 2 treatment arms.\\n\\nARM A2: Patients undergo standard-dose photon irradiation using 3D-CRT or IMRT as in Arm A1.\\n\\nARM C: Patients undergo dose-escalated and -intensified proton beam radiation therapy QD, 5 days a week for a total of 30 fractions.\\n\\nIn all treatment arms, patients receive temozolomide orally (PO) QD on days 1-49 of radiation therapy. Beginning 4 weeks later, patients receive temozolomide PO QD on days 1-5. Treatment repeats every 28 days for up to 12 cycles in the absence of disease progression or unacceptable toxicity.\\n\\nAfter completion of study treatment, patients are followed up every 3 months for 1 year, every 4 months for 1 year, and then every 6 months thereafter. This randomized phase II trial studies how well dose-escalated photon intensity-modulated radiation therapy (IMRT) or proton beam radiation therapy works compared with standard-dose radiation therapy when given with temozolomide in patients with newly diagnosed glioblastoma. Radiation therapy uses high-energy x-rays and other types of radiation to kill tumor cells and shrink tumors. Specialized radiation therapy that delivers a high dose of radiation directly to the tumor may kill more tumor cells and cause less damage to normal tissue. Drugs, such as temozolomide, may make tumor cells more sensitive to radiation therapy. It is not yet known whether dose-escalated photon IMRT or proton beam radiation therapy is more effective than standard-dose radiation therapy with temozolomide in treating glioblastoma. ACTIVE_NOT_RECRUITING [{\\'measure\\': \\'Overall survival (OS) compared between dose-escalated and -intensified photon IMRT or proton beam therapy with concomitant and adjuvant temozolomide and the standard-dose photon irradiation with concomitant and adjuvant temozolomide\\', \\'description\\': \\'OS rate will be estimated using the Kaplan-Meier method, and differences between treatment arms will be tested in a stratified log-rank test, consistent with the stratified randomization. The OS rates by MGMT, recursive partitioning analysis (RPA) class and other prognostic factors will be estimated by Kaplan-Meier methods and compared using the log-rank test. Multivariate analyses with the Cox proportional hazard model for OS will be performed to assess the treatment effect adjusting for patient-specific risk factors.\\', \\'timeFrame\\': \\'Date of randomization to the date of death due to any cause, assessed up to 5 years\\'}] [{\\'measure\\': \\'OS when compared between dose-escalated and -intensified photon IMRT to dose-escalated and -intensified proton beam therapy\\', \\'description\\': \\'If the instrumental variable assumptions hold, OS rate will be estimated using the Kaplan-Meier method, and differences between treatment arms will be tested in the log-rank test.\\', \\'timeFrame\\': \\'Date of randomization to the date of death, assessed up to 5 years\\'}, {\\'measure\\': \\'Progression-free survival (PFS)\\', \\'description\\': \\'PFS rates will be estimated using the Kaplan-Meier method and comparisons between treatment arms will be made in the same manner as for OS.\\', \\'timeFrame\\': \\'Date of randomization to the date of progression or death, assessed up to 5 years\\'}, {\\'measure\\': \\'Incidence of treatment-related toxicity, as measured by the Common Terminology Criteria for Adverse Events version 4\\', \\'description\\': \\'Differences in observed severities of toxicities (grade 3+) between groups will be estimated using an exact binomial distribution together with 95% confidence interval. The difference between the 2 groups will be tested using a chi square test. If the instrumental variable assumptions hold the experimental arms will also be compared.\\', \\'timeFrame\\': \\'Up to 5 years\\'}, {\\'measure\\': \\'Change in perceived cognitive function, as measured by M.D. Anderson Symptom Inventory Brain Tumor\\', \\'description\\': \\'The change from baseline to each follow-up time point for the perceived cognitive symptom severity score will each be compared using a t-test with alpha=0.05, or Wilcoxon test if the data is not normally distributed, between treatment arms within each group. If the instrumental variable assumptions hold and the perceived cognitive function is significantly different within both groups, a test will be performed to compare between the 2 experimental arms.\\', \\'timeFrame\\': \\'Baseline to up to 60 weeks\\'}, {\\'measure\\': \"Change in neurocognitive function, as measured by Hopkins\\' Verbal Learning Test-Revised, Trail Making Test Parts A and B, and Controlled Oral Word Association Test\", \\'description\\': \\'The change from baseline to each follow-up time point for the perceived Clinical Trial Battery (CTB) composite score will each be compared using a t-test with alpha=0.05, or Wilcoxon test if the data is not normally distributed, between treatment arms within each group. If the instrumental variable assumptions hold and the CTB composite score is significantly different within both groups, a test will be performed to compare between the 2 experimental arms.\\', \\'timeFrame\\': \\'Baseline to up to 60 weeks\\'}, {\\'measure\\': \\'Change in CD4 lymphopenia count\\', \\'description\\': \\'The change from baseline to the completion of radiation will be compared between the control and experimental arms in each group using a t-test. If the instrumental variable assumptions hold, then it will be compared between the experimental arms. A repeated measures analysis, using a mixed effects model, will be used to assess the change of CD4 lymphopenia across time. CD4 count at 2 months after beginning therapy (dichotomized at 200) was shown to be prognostic of OS. This will be assessed here based on the CD4 count at the completion of chemoradiation which matches best to 2-months.\\', \\'timeFrame\\': \\'Baseline to up to 5 years\\'}, {\\'measure\\': \\'Use of magnetic resonance diffusion and perfusion imaging to differentiate between tumor progression and pseudo-progression\\', \\'description\\': \\'Retrospective analysis will be performed to evaluate and refine the method and the threshold cut-off point determined from our previous single institute data set to determine progression using the first one-third of the patient data collected in this trial. If the initial analysis supports our preliminary results with sufficient high sensitivity and specificity, e.g., 80% specificity and 90% sensitivity or higher, results will be validated using the remaining two-thirds of the imaging data.\\', \\'timeFrame\\': \\'Up to 5 years\\'}] ADULT, OLDER_ADULT [\\'Adult Giant Cell Glioblastoma\\', \\'Adult Glioblastoma\\', \\'Adult Gliosarcoma\\'] [{\\'name\\': \\'Minesh Mehta, MD\\', \\'affiliation\\': \\'NRG Oncology\\', \\'role\\': \\'PRINCIPAL_INVESTIGATOR\\'}, {\\'name\\': \\'Vinai Gondi, MD\\', \\'affiliation\\': \\'NRG Oncology\\', \\'role\\': \\'PRINCIPAL_INVESTIGATOR\\'}] [{\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}, {\\'facility\\': \\'Loma Linda University Medical Center\\', \\'city\\': \\'Loma Linda\\', \\'state\\': \\'California\\', \\'country\\': \\'United States\\'}] ')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    Please provide a concise answer. \\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain =  qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here\n",
      " are\n",
      " some\n",
      " clinical\n",
      " trials\n",
      " related\n",
      " to\n",
      " brain\n",
      " tumors\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " **\n",
      "N\n",
      "CT\n",
      "000\n",
      "306\n",
      "28\n",
      "**\n",
      ":\n",
      " A\n",
      " Phase\n",
      " III\n",
      " Random\n",
      "ized\n",
      " Trial\n",
      " of\n",
      " the\n",
      " Role\n",
      " of\n",
      " Whole\n",
      " Brain\n",
      " Radiation\n",
      " Therapy\n",
      " in\n",
      " Addition\n",
      " to\n",
      " Radios\n",
      "urgery\n",
      " in\n",
      " the\n",
      " Management\n",
      " of\n",
      " Patients\n",
      " With\n",
      " One\n",
      " to\n",
      " Three\n",
      " Cere\n",
      "bral\n",
      " Met\n",
      "ast\n",
      "ases\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Objective\n",
      "**\n",
      ":\n",
      " Compare\n",
      " overall\n",
      " survival\n",
      ",\n",
      " time\n",
      " to\n",
      " CNS\n",
      " failure\n",
      ",\n",
      " quality\n",
      " of\n",
      " life\n",
      ",\n",
      " and\n",
      " neuro\n",
      "c\n",
      "ognitive\n",
      " status\n",
      " in\n",
      " patients\n",
      " treated\n",
      " with\n",
      " radios\n",
      "urgery\n",
      " with\n",
      " or\n",
      " without\n",
      " whole\n",
      " brain\n",
      " radi\n",
      "otherapy\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Status\n",
      "**\n",
      ":\n",
      " Completed\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " **\n",
      "N\n",
      "CT\n",
      "055\n",
      "761\n",
      "03\n",
      "**\n",
      ":\n",
      " Long\n",
      "itud\n",
      "inal\n",
      " Pros\n",
      "pective\n",
      " Study\n",
      " of\n",
      " Neuro\n",
      "c\n",
      "ognitive\n",
      " Outcomes\n",
      " and\n",
      " Mult\n",
      "im\n",
      "odal\n",
      " Quant\n",
      "itative\n",
      " Neuro\n",
      "im\n",
      "aging\n",
      " Outcomes\n",
      " in\n",
      " Primary\n",
      " Brain\n",
      " Tum\n",
      "or\n",
      " Patients\n",
      " Receiving\n",
      " Brain\n",
      " Radi\n",
      "otherapy\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Objective\n",
      "**\n",
      ":\n",
      " Examine\n",
      " radiation\n",
      "-induced\n",
      " imaging\n",
      " changes\n",
      " in\n",
      " normal\n",
      " brain\n",
      " tissue\n",
      " over\n",
      " time\n",
      " and\n",
      " correlate\n",
      " these\n",
      " with\n",
      " neuro\n",
      "c\n",
      "ognitive\n",
      " outcomes\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Status\n",
      "**\n",
      ":\n",
      " Recruiting\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " **\n",
      "N\n",
      "CT\n",
      "035\n",
      "503\n",
      "91\n",
      "**\n",
      ":\n",
      " A\n",
      " Phase\n",
      " III\n",
      " Trial\n",
      " of\n",
      " S\n",
      "tere\n",
      "ot\n",
      "actic\n",
      " Radios\n",
      "urgery\n",
      " Compared\n",
      " With\n",
      " Hipp\n",
      "oc\n",
      "amp\n",
      "al\n",
      "-A\n",
      "void\n",
      "ant\n",
      " Whole\n",
      " Brain\n",
      " Radi\n",
      "otherapy\n",
      " (\n",
      "HA\n",
      "-W\n",
      "B\n",
      "RT\n",
      ")\n",
      " Plus\n",
      " Mem\n",
      "ant\n",
      "ine\n",
      " for\n",
      " \n",
      "5\n",
      " or\n",
      " More\n",
      " Brain\n",
      " Met\n",
      "ast\n",
      "ases\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Objective\n",
      "**\n",
      ":\n",
      " Compare\n",
      " the\n",
      " effects\n",
      " of\n",
      " stere\n",
      "ot\n",
      "actic\n",
      " radios\n",
      "urgery\n",
      " (\n",
      "S\n",
      "RS\n",
      ")\n",
      " versus\n",
      " HA\n",
      "-W\n",
      "B\n",
      "RT\n",
      " plus\n",
      " mem\n",
      "ant\n",
      "ine\n",
      " on\n",
      " brain\n",
      " metast\n",
      "ases\n",
      ",\n",
      " overall\n",
      " survival\n",
      ",\n",
      " and\n",
      " quality\n",
      " of\n",
      " life\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Status\n",
      "**\n",
      ":\n",
      " Recruiting\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " **\n",
      "N\n",
      "CT\n",
      "021\n",
      "790\n",
      "86\n",
      "**\n",
      ":\n",
      " Random\n",
      "ized\n",
      " Phase\n",
      " II\n",
      " Trial\n",
      " of\n",
      " Hyp\n",
      "of\n",
      "raction\n",
      "ated\n",
      " Dose\n",
      "-E\n",
      "sc\n",
      "al\n",
      "ated\n",
      " Photon\n",
      " IM\n",
      "RT\n",
      " or\n",
      " Proton\n",
      " Beam\n",
      " Therapy\n",
      " Vers\n",
      "us\n",
      " Conventional\n",
      " Photon\n",
      " Irr\n",
      "adi\n",
      "ation\n",
      " With\n",
      " Con\n",
      "com\n",
      "itant\n",
      " and\n",
      " Ad\n",
      "ju\n",
      "vant\n",
      " Tem\n",
      "oz\n",
      "ol\n",
      "om\n",
      "ide\n",
      " in\n",
      " Patients\n",
      " With\n",
      " Newly\n",
      " Diagn\n",
      "osed\n",
      " Gli\n",
      "oblast\n",
      "oma\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Objective\n",
      "**\n",
      ":\n",
      " Determine\n",
      " if\n",
      " dose\n",
      "-es\n",
      "cal\n",
      "ated\n",
      " photon\n",
      " IM\n",
      "RT\n",
      " or\n",
      " proton\n",
      " beam\n",
      " therapy\n",
      " improves\n",
      " overall\n",
      " survival\n",
      " compared\n",
      " to\n",
      " standard\n",
      "-dose\n",
      " radiation\n",
      " therapy\n",
      " with\n",
      " tem\n",
      "oz\n",
      "ol\n",
      "om\n",
      "ide\n",
      ".\n",
      "\n",
      "  \n",
      " -\n",
      " **\n",
      "Status\n",
      "**\n",
      ":\n",
      " Active\n",
      ",\n",
      " not\n",
      " recruiting\n",
      "\n",
      "\n",
      "\n",
      "These\n",
      " trials\n",
      " cover\n",
      " various\n",
      " aspects\n",
      " of\n",
      " brain\n",
      " tumor\n",
      " treatment\n",
      ",\n",
      " including\n",
      " radiation\n",
      " therapy\n",
      ",\n",
      " neuro\n",
      "c\n",
      "ognitive\n",
      " outcomes\n",
      ",\n",
      " and\n",
      " the\n",
      " effectiveness\n",
      " of\n",
      " different\n",
      " therapeutic\n",
      " approaches\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, chunk in enumerate(qa_chain.stream({\"input\": input, \"chat_history\": [], \"context\":docs})):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are some clinical trials related to brain tumors:\\n\\n1. **NCT00030628**: A Phase III Randomized Trial of the Role of Whole Brain Radiation Therapy in Addition to Radiosurgery in the Management of Patients With One to Three Cerebral Metastases.\\n   - **Objective**: Compare the effectiveness of radiosurgery with or without whole-brain radiation therapy in treating brain metastases.\\n   - **Status**: Completed.\\n\\n2. **NCT05576103**: Longitudinal Prospective Study of Neurocognitive Outcomes and Multimodal Quantitative Neuroimaging Outcomes in Primary Brain Tumor Patients Receiving Brain Radiotherapy.\\n   - **Objective**: Examine radiation-induced imaging changes in normal brain tissue over time and correlate these with neurocognitive outcomes.\\n   - **Status**: Recruiting.\\n\\n3. **NCT03550391**: A Phase III Trial of Stereotactic Radiosurgery Compared With Hippocampal-Avoidant Whole Brain Radiotherapy (HA-WBRT) Plus Memantine for 5 or More Brain Metastases.\\n   - **Objective**: Compare the effects of stereotactic radiosurgery (SRS) versus HA-WBRT plus memantine on brain metastases.\\n   - **Status**: Recruiting.\\n\\n4. **NCT02179086**: Randomized Phase II Trial of Hypofractionated Dose-Escalated Photon IMRT or Proton Beam Therapy Versus Conventional Photon Irradiation With Concomitant and Adjuvant Temozolomide in Patients With Newly Diagnosed Glioblastoma.\\n   - **Objective**: Compare dose-escalated photon IMRT or proton beam radiation therapy with standard-dose radiation therapy when given with temozolomide in patients with newly diagnosed glioblastoma.\\n   - **Status**: Active, not recruiting.\\n\\nThese trials cover various aspects of brain tumor treatment, including radiosurgery, whole-brain radiation therapy, neurocognitive outcomes, and advanced radiation techniques.', response_metadata={'token_usage': {'completion_tokens': 403, 'prompt_tokens': 10806, 'total_tokens': 11209}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d3df658-3eb5-40f6-9d83-27672abeae7b-0', usage_metadata={'input_tokens': 10806, 'output_tokens': 403, 'total_tokens': 11209})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [{'role': 'user', 'content': 'Hi! How are you?'}, {'role': 'assistant', 'content': 'Hello, How can I help you?'}, {'role': 'user', 'content': 'Could you tell me few trials related to brain tumor?'}, {'role': 'assistant', 'content': 'Based on the contexts provided, here are a few clinical trials related to brain tumors:\\n\\n1. **Trial Title:** Phase II Trial of the Immune Checkpoint Inhibitor Nivolumab in Patients With Recurrent Select Rare CNS Cancers\\n   - **NCT ID:** NCT03173950\\n   - **Sponsor:** National Cancer Institute (NCI)\\n   - **Objective:** Evaluate the efficacy of the immune checkpoint inhibitor Nivolumab in patients with recurrent rare central nervous system neoplasms.\\n   - **Eligibility:** Patients with various rare CNS cancers, aged 18 or above, with progressive tumor growth, and specific laboratory parameters within normal range.\\n   - **Status:** Recruiting\\n\\n2. **Trial Title:** A Randomized Phase III Trial of Pre-Operative Compared to Post-Operative Stereotactic Radiosurgery in Patients With Resectable Brain Metastases\\n   - **NCT ID:** NCT05438212\\n   - **Sponsor:** NRG Oncology\\n   - **Objective:** Compare the addition of stereotactic radiosurgery before or after surgery in patients with brain metastases to assess the impact on overall survival and progression-free survival.\\n   - **Eligibility:** Patients with resectable brain metastases, who meet specific criteria related to tumor size and location.\\n   - **Status:** Recruiting\\n\\n3. **Trial Title:** ONC201 for the Treatment of Newly Diagnosed H3 K27M-mutant Diffuse Glioma Following Completion of Radiotherapy: A Randomized, Double-Blind, Placebo-Controlled, Multicenter Study\\n   - **NCT ID:** NCT05580562\\n   - **Sponsor:** Chimerix\\n   - **Objective:** Assess whether treatment with ONC201 following radiotherapy extends overall survival and progression-free survival in patients with newly diagnosed H3 K27M-mutant diffuse glioma.\\n   - **Eligibility:** Patients diagnosed with H3 K27M-mutant diffuse glioma who have completed frontline radiotherapy.\\n   - **Status:** Recruiting\\n\\n4. **Trial Title:** Phase II Trial of BRAF/MEK Inhibitors in Papillary Craniopharyngiomas\\n   - **NCT ID:** NCT03224767\\n   - **Sponsor:** Alliance for Clinical Trials in Oncology\\n   - **Objective:** Evaluate the activity of BRAF and MEK inhibitor combination in untreated and previously treated papillary craniopharyngiomas.\\n   - **Eligibility:** Patients with histologically proven papillary craniopharyngioma with a positive BRAF V600E mutation.\\n   - **Status:** Recruiting\\n\\nThese trials aim to contribute to the understanding and treatment of various brain tumors, offering new insights and potential therapeutic options for patients.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"Can you give me information about the first trial you mentioned?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_msg_1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"What are everyone's favorite colors:\\n\\n{context}\")]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "\n",
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = history_aware_retriever.invoke({\"input\":\"Can you give me information about the first trial you mentioned?\",\"chat_history\":chat_history})\n",
    "question_answer_chain.invoke({\"input\":\"Can you give me information about the first trial you mentioned?\",\"chat_history\":chat_history, \"context\":results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([(\"system\", contextualize_q_system_prompt),MessagesPlaceholder(\"chat_history\"),(\"human\", \"{input}\")])\n",
    "\n",
    "chain= contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\":\"Could you give me information about the first trial mentioned?\", \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = contextualize_q_prompt.invoke({\"input\":\"Can you give me information about the Nivolumab trial you mentioned?\", \"chat_history\":chat_history})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), \n",
    "\n",
    "SystemMessage(content='\\n\\n        You are a helpful Assistant who answers to users questions based on multiple contexts given to you.\\n\\n        Keep your answer creative.\\n        \\n        Please take into account the previous messages as well.\\n        \\n        Make sure to citation for the answer from metadata.\\n            \\n        Reply to greetings messages.\\n    '), \n",
    "HumanMessage(content='Hi! How are you?'), AIMessage(content='Hello, How can I help you?'), HumanMessage(content='Could you tell me few trials related to brain tumor?'), AIMessage(content='Based on the contexts provided, here are a few clinical trials related to brain tumors:\\n\\n1. **Trial Title:** Phase II Trial of the Immune Checkpoint Inhibitor Nivolumab in Patients With Recurrent Select Rare CNS Cancers\\n   - **NCT ID:** NCT03173950\\n   - **Sponsor:** National Cancer Institute (NCI)\\n   - **Objective:** Evaluate the efficacy of the immune checkpoint inhibitor Nivolumab in patients with recurrent rare central nervous system neoplasms.\\n   - **Eligibility:** Patients with various rare CNS cancers, aged 18 or above, with progressive tumor growth, and specific laboratory parameters within normal range.\\n   - **Status:** Recruiting\\n\\n2. **Trial Title:** A Randomized Phase III Trial of Pre-Operative Compared to Post-Operative Stereotactic Radiosurgery in Patients With Resectable Brain Metastases\\n   - **NCT ID:** NCT05438212\\n   - **Sponsor:** NRG Oncology\\n   - **Objective:** Compare the addition of stereotactic radiosurgery before or after surgery in patients with brain metastases to assess the impact on overall survival and progression-free survival.\\n   - **Eligibility:** Patients with resectable brain metastases, who meet specific criteria related to tumor size and location.\\n   - **Status:** Recruiting\\n\\n3. **Trial Title:** ONC201 for the Treatment of Newly Diagnosed H3 K27M-mutant Diffuse Glioma Following Completion of Radiotherapy: A Randomized, Double-Blind, Placebo-Controlled, Multicenter Study\\n   - **NCT ID:** NCT05580562\\n   - **Sponsor:** Chimerix\\n   - **Objective:** Assess whether treatment with ONC201 following radiotherapy extends overall survival and progression-free survival in patients with newly diagnosed H3 K27M-mutant diffuse glioma.\\n   - **Eligibility:** Patients diagnosed with H3 K27M-mutant diffuse glioma who have completed frontline radiotherapy.\\n   - **Status:** Recruiting\\n\\n4. **Trial Title:** Phase II Trial of BRAF/MEK Inhibitors in Papillary Craniopharyngiomas\\n   - **NCT ID:** NCT03224767\\n   - **Sponsor:** Alliance for Clinical Trials in Oncology\\n   - **Objective:** Evaluate the activity of BRAF and MEK inhibitor combination in untreated and previously treated papillary craniopharyngiomas.\\n   - **Eligibility:** Patients with histologically proven papillary craniopharyngioma with a positive BRAF V600E mutation.\\n   - **Status:** Recruiting\\n\\nThese trials aim to contribute to the understanding and treatment of various brain tumors, offering new insights and potential therapeutic options for patients.'), HumanMessage(content='Can you give me information about the Nivolumab trial you mentioned?')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), SystemMessage(content='\\n\\n        You are a helpful Assistant who answers to users questions based on multiple contexts given to you.\\n\\n        Keep your answer creative.\\n        \\n        Please take into account the previous messages as well.\\n        \\n        Make sure to citation for the answer from metadata.\\n            \\n        Reply to greetings messages.\\n    '), HumanMessage(content='Hi! How are you?'), AIMessage(content='Hello, How can I help you?'), HumanMessage(content='Could you tell me few trials related to brain tumor?'), AIMessage(content='Based on the contexts provided, here are a few clinical trials related to brain tumors:\\n\\n1. **Trial Title:** Phase II Trial of the Immune Checkpoint Inhibitor Nivolumab in Patients With Recurrent Select Rare CNS Cancers\\n   - **NCT ID:** NCT03173950\\n   - **Sponsor:** National Cancer Institute (NCI)\\n   - **Objective:** Evaluate the efficacy of the immune checkpoint inhibitor Nivolumab in patients with recurrent rare central nervous system neoplasms.\\n   - **Eligibility:** Patients with various rare CNS cancers, aged 18 or above, with progressive tumor growth, and specific laboratory parameters within normal range.\\n   - **Status:** Recruiting\\n\\n2. **Trial Title:** A Randomized Phase III Trial of Pre-Operative Compared to Post-Operative Stereotactic Radiosurgery in Patients With Resectable Brain Metastases\\n   - **NCT ID:** NCT05438212\\n   - **Sponsor:** NRG Oncology\\n   - **Objective:** Compare the addition of stereotactic radiosurgery before or after surgery in patients with brain metastases to assess the impact on overall survival and progression-free survival.\\n   - **Eligibility:** Patients with resectable brain metastases, who meet specific criteria related to tumor size and location.\\n   - **Status:** Recruiting\\n\\n3. **Trial Title:** ONC201 for the Treatment of Newly Diagnosed H3 K27M-mutant Diffuse Glioma Following Completion of Radiotherapy: A Randomized, Double-Blind, Placebo-Controlled, Multicenter Study\\n   - **NCT ID:** NCT05580562\\n   - **Sponsor:** Chimerix\\n   - **Objective:** Assess whether treatment with ONC201 following radiotherapy extends overall survival and progression-free survival in patients with newly diagnosed H3 K27M-mutant diffuse glioma.\\n   - **Eligibility:** Patients diagnosed with H3 K27M-mutant diffuse glioma who have completed frontline radiotherapy.\\n   - **Status:** Recruiting\\n\\n4. **Trial Title:** Phase II Trial of BRAF/MEK Inhibitors in Papillary Craniopharyngiomas\\n   - **NCT ID:** NCT03224767\\n   - **Sponsor:** Alliance for Clinical Trials in Oncology\\n   - **Objective:** Evaluate the activity of BRAF and MEK inhibitor combination in untreated and previously treated papillary craniopharyngiomas.\\n   - **Eligibility:** Patients with histologically proven papillary craniopharyngioma with a positive BRAF V600E mutation.\\n   - **Status:** Recruiting\\n\\nThese trials aim to contribute to the understanding and treatment of various brain tumors, offering new insights and potential therapeutic options for patients.'), HumanMessage(content='Can you give me information about the Nivolumab trial you mentioned?')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LCEL Making a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from template import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def get_chain(vectorstore):\n",
    "\n",
    "\n",
    "    model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1, api_key = api_key)\n",
    "\n",
    "\n",
    "    def _combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "            doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "            return document_separator.join(doc_strings)\n",
    "\n",
    "    _inputs = RunnableParallel(\n",
    "        standalone_question=RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | model\n",
    "        | StrOutputParser(),\n",
    "    )\n",
    "    _context = {\n",
    "        \"context\": itemgetter(\"standalone_question\")\n",
    "        | vectorstore.as_retriever()\n",
    "        | _combine_documents,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    conversational_qa_chain = _inputs | _context \n",
    "    \n",
    "    return conversational_qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def get_chain(vectorstore):\n",
    "\n",
    "    model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1, api_key = api_key)\n",
    "\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history, mention the required details in the question itself, also mention the NCT ID if present in the previous trial.\\\n",
    "    Just return the standalone question, Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
    "    question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "vectorstore = FAISS.load_local(\"../database/vectorDB/mainDB\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "rag_chain = get_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"chat_history.json\", \"r\") as file:\n",
    "    chat_history_json = file.read()\n",
    "\n",
    "chat_history = json.loads(chat_history_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "def convert_messages_list(messages_list):\n",
    "    langchain_messages = []\n",
    "\n",
    "    for messages_dict in messages_list:\n",
    "        role = messages_dict.get('role')\n",
    "        content = messages_dict.get(\"content\")\n",
    "\n",
    "        if role == \"user\":\n",
    "            langchain_messages.append(HumanMessage(content = content))\n",
    "        elif role == \"assistant\":\n",
    "            langchain_messages.append(AIMessage(content = content))\n",
    "\n",
    "    return langchain_messages\n",
    "\n",
    "langchain_messages = convert_messages_list(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langchain_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Can you suggest trials?\"\n",
    "count=2\n",
    "for chunk in rag_chain.stream({\"chat_history\": langchain_messages, \"input\": question}):\n",
    "    if count!=0:\n",
    "        count-=1\n",
    "    else:\n",
    "        print(chunk[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hi, how are you?\"),\n",
    "    AIMessage(content=\"Good, how are you?\"),\n",
    "]\n",
    "get_buffer_string(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = get_buffer_string(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identify:\n",
    "\n",
    "    def __init__ (self, chat_history, input):\n",
    "\n",
    "        self.chat_history = chat_history\n",
    "        self.input = input\n",
    "\n",
    "    def identify_chain(self):\n",
    "\n",
    "        system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "        which might reference context in the chat history,\\\n",
    "        please answer if the user question is related to clinical trials or studies?\\ \n",
    "        Just answer \"Yes\" or \"No\"\\\n",
    "        \"\"\"\n",
    "        main_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1, api_key = api_key)\n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qa_chain = qa_prompt | main_model\n",
    "        print(\"Till here fine\")\n",
    "        self.result = qa_chain.invoke({\"input\": self.input, \"chat_history\": self.chat_history}).content\n",
    "        print(self.result)\n",
    "\n",
    "    def cosine_similarity(self, a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    def determine_answer(self):\n",
    "        \n",
    "        embeddings = openai.embeddings.create(input=[\"Yes\", \"No\", self.result], model=\"text-embedding-ada-002\")\n",
    "        print(embeddings.data)\n",
    "        vector1 = embeddings.data[0].embedding\n",
    "        vector2 = embeddings.data[1].embedding\n",
    "        vector3 = embeddings.data[2].embedding\n",
    "    \n",
    "        score_yes = self.cosine_similarity(vector1, vector3)\n",
    "        score_no = self.cosine_similarity( vector2, vector3)\n",
    "        return True if score_yes > score_no else False\n",
    "\n",
    "def question_relatable(chat_history, input):\n",
    "    print(\"I am here?\")\n",
    "    obj = Identify(chat_history, input)\n",
    "    print(\"problem\")\n",
    "    obj.identify_chain()\n",
    "    print(\"problem2\")\n",
    "    result = obj.determine_answer()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_relatable(chat_history, input = \"Can you tell me trials related to brain tumor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
